{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import time\n",
        "from sklearn.feature_extraction.text import HashingVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.utils import resample\n",
        "from sklearn.metrics import pairwise_distances_argmin_min\n",
        "from sklearn.cluster import MiniBatchKMeans\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "train_texts = pd.read_pickle('train.pkl')\n",
        "labels = pd.read_pickle('labels.pkl')\n",
        "df = pd.DataFrame({\n",
        "    'text': train_texts,\n",
        "    'label': labels\n",
        "})\n",
        "\n",
        "print(\"Combined DataFrame shape:\", df.shape)\n",
        "print(\"Class distribution:\\n\", df['label'].value_counts())\n",
        "\n",
        "def check_model_metrics(model, test_data, test_targets):\n",
        "    y_pred = model.predict(test_data)\n",
        "\n",
        "    print(\"ACCURACY:\")\n",
        "    print(metrics.accuracy_score(test_targets, y_pred) * 100)\n",
        "\n",
        "    print(\"\\nCONFUSION MATRIX\")\n",
        "    print(confusion_matrix(test_targets, y_pred))\n",
        "\n",
        "    print(\"\\nCLASSIFICATION REPORT\")\n",
        "    print(classification_report(test_targets, y_pred))\n",
        "\n",
        "df_train, df_test = train_test_split(df, test_size=0.2, stratify=df['label'], random_state=42)\n",
        "\n",
        "print(\"\\nTraining Data Shape:\", df_train.shape)\n",
        "print(\"Test Data Shape:\", df_test.shape)\n",
        "\n",
        "def train_full_data(df_train, df_test):\n",
        "    vectorizer = HashingVectorizer(n_features=2**18)\n",
        "    X_train = vectorizer.fit_transform(df_train['text'])\n",
        "    y_train = df_train['label']\n",
        "\n",
        "    X_test = vectorizer.transform(df_test['text'])\n",
        "    y_test = df_test['label']\n",
        "\n",
        "    start_time = time.time()\n",
        "\n",
        "    clf = LogisticRegression(max_iter=1000, n_jobs=-1)\n",
        "    clf.fit(X_train, y_train)\n",
        "\n",
        "    end_time = time.time()\n",
        "    print(\"Direct training time (s):\", end_time - start_time)\n",
        "\n",
        "    # Evaluate the model\n",
        "    check_model_metrics(clf, X_test, y_test)\n",
        "\n",
        "def train_downsampled_data(df_train, df_test):\n",
        "    df_downsampled = resample(\n",
        "        df_train,\n",
        "        replace=False,\n",
        "        n_samples=100000,\n",
        "        stratify=df_train['label'],\n",
        "        random_state=42\n",
        "    )\n",
        "\n",
        "    vectorizer = HashingVectorizer(n_features=2**8)\n",
        "\n",
        "    X_train = vectorizer.fit_transform(df_downsampled['text'])\n",
        "    y_train = df_downsampled['label']\n",
        "\n",
        "    X_test = vectorizer.transform(df_test['text'])\n",
        "    y_test = df_test['label']\n",
        "\n",
        "    start_time = time.time()\n",
        "    clf = LogisticRegression(max_iter=1000, n_jobs=-1)\n",
        "    clf.fit(X_train, y_train)\n",
        "\n",
        "    end_time = time.time()\n",
        "    print(\"Downsampled training time (s):\", end_time - start_time)\n",
        "\n",
        "    check_model_metrics(clf, X_test, y_test)\n",
        "\n",
        "def train_with_coreset(df_train, df_test):\n",
        "    df_sampled = resample(\n",
        "        df_train,\n",
        "        replace=False,\n",
        "        n_samples=100000,\n",
        "        stratify=df_train['label'],\n",
        "        random_state=42\n",
        "    )\n",
        "\n",
        "    vectorizer = HashingVectorizer(n_features=2**10)\n",
        "    X_sampled = vectorizer.fit_transform(df_sampled['text'])\n",
        "    y_sampled = df_sampled['label']\n",
        "\n",
        "    X_test = vectorizer.transform(df_test['text'])\n",
        "    y_test = df_test['label']\n",
        "\n",
        "    n_clusters = 1000\n",
        "    print(f\"Constructing coreset with {n_clusters} clusters...\")\n",
        "    start_time = time.time()\n",
        "\n",
        "    kmeans = MiniBatchKMeans(n_clusters=n_clusters, batch_size=10000, random_state=42)\n",
        "    kmeans.fit(X_sampled)\n",
        "\n",
        "    closest, _ = pairwise_distances_argmin_min(kmeans.cluster_centers_, X_sampled)\n",
        "    coreset_indices = np.unique(closest)\n",
        "\n",
        "    X_coreset = X_sampled[coreset_indices]\n",
        "    y_coreset = y_sampled.iloc[coreset_indices].reset_index(drop=True)\n",
        "\n",
        "    coreset_construction_time = time.time() - start_time\n",
        "    print(f\"Coreset construction time (s): {coreset_construction_time}\")\n",
        "    print(f\"Coreset size: {X_coreset.shape[0]}\")\n",
        "    print(f\"Unique classes in coreset: {y_coreset.nunique()}\")\n",
        "\n",
        "    if y_coreset.nunique() < 2:\n",
        "        raise ValueError(\"Coreset contains only one class. Adjust the number of clusters or check data diversity.\")\n",
        "\n",
        "    start_time = time.time()\n",
        "    clf = LogisticRegression(max_iter=1000, n_jobs=-1)\n",
        "    clf.fit(X_coreset, y_coreset)\n",
        "    end_time = time.time()\n",
        "    print(f\"Coreset training time (s): {end_time - start_time}\")\n",
        "\n",
        "    check_model_metrics(clf, X_test, y_test)\n",
        "\n",
        "print(\"\\nExperiment 1: Direct HashingVectorizer\")\n",
        "train_full_data(df_train, df_test)\n",
        "\n",
        "print(\"\\nExperiment 2: Stratified Downsampling and Reduced Features\")\n",
        "train_downsampled_data(df_train, df_test)\n",
        "\n",
        "print(\"\\nExperiment 3: Proper Coreset Construction\")\n",
        "train_with_coreset(df_train, df_test)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RwNaoFEPep_P",
        "outputId": "7fcf04c1-e2f6-464f-ac91-0facb9e4af42"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Combined DataFrame shape: (1517041, 2)\n",
            "Class distribution:\n",
            " label\n",
            "0    763629\n",
            "4    753412\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Training Data Shape: (1213632, 2)\n",
            "Test Data Shape: (303409, 2)\n",
            "\n",
            "Experiment 1: Direct HashingVectorizer\n",
            "Direct training time (s): 23.444284915924072\n",
            "ACCURACY:\n",
            "77.68292964282536\n",
            "\n",
            "CONFUSION MATRIX\n",
            "[[116102  36624]\n",
            " [ 31088 119595]]\n",
            "\n",
            "CLASSIFICATION REPORT\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.79      0.76      0.77    152726\n",
            "           4       0.77      0.79      0.78    150683\n",
            "\n",
            "    accuracy                           0.78    303409\n",
            "   macro avg       0.78      0.78      0.78    303409\n",
            "weighted avg       0.78      0.78      0.78    303409\n",
            "\n",
            "\n",
            "Experiment 2: Stratified Downsampling and Reduced Features\n",
            "Downsampled training time (s): 0.9366509914398193\n",
            "ACCURACY:\n",
            "64.94303069454104\n",
            "\n",
            "CONFUSION MATRIX\n",
            "[[98398 54328]\n",
            " [52038 98645]]\n",
            "\n",
            "CLASSIFICATION REPORT\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.65      0.64      0.65    152726\n",
            "           4       0.64      0.65      0.65    150683\n",
            "\n",
            "    accuracy                           0.65    303409\n",
            "   macro avg       0.65      0.65      0.65    303409\n",
            "weighted avg       0.65      0.65      0.65    303409\n",
            "\n",
            "\n",
            "Experiment 3: Proper Coreset Construction\n",
            "Constructing coreset with 1000 clusters...\n",
            "Coreset construction time (s): 46.37545680999756\n",
            "Coreset size: 866\n",
            "Unique classes in coreset: 2\n",
            "Coreset training time (s): 0.013052701950073242\n",
            "ACCURACY:\n",
            "64.5946560583239\n",
            "\n",
            "CONFUSION MATRIX\n",
            "[[ 88944  63782]\n",
            " [ 43641 107042]]\n",
            "\n",
            "CLASSIFICATION REPORT\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.67      0.58      0.62    152726\n",
            "           4       0.63      0.71      0.67    150683\n",
            "\n",
            "    accuracy                           0.65    303409\n",
            "   macro avg       0.65      0.65      0.64    303409\n",
            "weighted avg       0.65      0.65      0.64    303409\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import time\n",
        "from sklearn.feature_extraction.text import HashingVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.utils import resample\n",
        "from sklearn.metrics import pairwise_distances_argmin_min\n",
        "from sklearn.cluster import MiniBatchKMeans\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "train_texts = pd.read_pickle('train.pkl')\n",
        "labels = pd.read_pickle('labels.pkl')\n",
        "df = pd.DataFrame({'text': train_texts, 'label': labels})\n",
        "\n",
        "print(\"Combined DataFrame shape:\", df.shape)\n",
        "print(\"Class distribution:\\n\", df['label'].value_counts())\n",
        "\n",
        "def check_model_metrics(model, test_data, test_targets):\n",
        "    y_pred = model.predict(test_data)\n",
        "    print(\"ACCURACY:\")\n",
        "    print(metrics.accuracy_score(test_targets, y_pred) * 100)\n",
        "    print(\"\\nCONFUSION MATRIX\")\n",
        "    print(confusion_matrix(test_targets, y_pred))\n",
        "    print(\"\\nCLASSIFICATION REPORT\")\n",
        "    print(classification_report(test_targets, y_pred))\n",
        "\n",
        "df_train, df_test = train_test_split(df, test_size=0.2, stratify=df['label'], random_state=42)\n",
        "\n",
        "print(\"\\nTraining Data Shape:\", df_train.shape)\n",
        "print(\"Test Data Shape:\", df_test.shape)\n",
        "\n",
        "def train_full_data(df_train, df_test):\n",
        "    vectorizer = HashingVectorizer(n_features=2**18)\n",
        "    X_train = vectorizer.fit_transform(df_train['text'])\n",
        "    y_train = df_train['label']\n",
        "    X_test = vectorizer.transform(df_test['text'])\n",
        "    y_test = df_test['label']\n",
        "    print(\"\\nExperiment 1: Direct HashingVectorizer\")\n",
        "    print(\"Training data shape:\", X_train.shape)\n",
        "    print(\"Number of features:\", X_train.shape[1])\n",
        "    start_time = time.time()\n",
        "    clf = LogisticRegression(max_iter=1000, n_jobs=-1)\n",
        "    clf.fit(X_train, y_train)\n",
        "    end_time = time.time()\n",
        "    print(\"Direct training time (s):\", end_time - start_time)\n",
        "    check_model_metrics(clf, X_test, y_test)\n",
        "\n",
        "def train_downsampled_data(df_train, df_test):\n",
        "    df_downsampled = resample(df_train, replace=False, n_samples=100000, stratify=df_train['label'], random_state=42)\n",
        "    vectorizer = HashingVectorizer(n_features=2**10)\n",
        "    X_train = vectorizer.fit_transform(df_downsampled['text'])\n",
        "    y_train = df_downsampled['label']\n",
        "    X_test = vectorizer.transform(df_test['text'])\n",
        "    y_test = df_test['label']\n",
        "    print(\"\\nExperiment 2: Stratified Downsampling and Reduced Features\")\n",
        "    print(\"Downsampled training data shape:\", X_train.shape)\n",
        "    print(\"Number of features:\", X_train.shape[1])\n",
        "    start_time = time.time()\n",
        "    clf = LogisticRegression(max_iter=1000, n_jobs=-1)\n",
        "    clf.fit(X_train, y_train)\n",
        "    end_time = time.time()\n",
        "    print(\"Downsampled training time (s):\", end_time - start_time)\n",
        "    check_model_metrics(clf, X_test, y_test)\n",
        "\n",
        "def train_with_coreset(df_train, df_test):\n",
        "    df_sampled = resample(df_train, replace=False, n_samples=100000, stratify=df_train['label'], random_state=42)\n",
        "    vectorizer = HashingVectorizer(n_features=2**10)\n",
        "    X_sampled = vectorizer.fit_transform(df_sampled['text'])\n",
        "    y_sampled = df_sampled['label']\n",
        "    X_test = vectorizer.transform(df_test['text'])\n",
        "    y_test = df_test['label']\n",
        "    n_clusters = 1000\n",
        "    print(\"\\nExperiment 3: Proper Coreset Construction\")\n",
        "    print(f\"Constructing coreset with {n_clusters} clusters...\")\n",
        "    print(\"Sampled data shape before coreset construction:\", X_sampled.shape)\n",
        "    print(\"Number of features:\", X_sampled.shape[1])\n",
        "    start_time = time.time()\n",
        "    kmeans = MiniBatchKMeans(n_clusters=n_clusters, batch_size=10000, random_state=42)\n",
        "    kmeans.fit(X_sampled)\n",
        "    closest, _ = pairwise_distances_argmin_min(kmeans.cluster_centers_, X_sampled)\n",
        "    coreset_indices = np.unique(closest)\n",
        "    X_coreset = X_sampled[coreset_indices]\n",
        "    y_coreset = y_sampled.iloc[coreset_indices].reset_index(drop=True)\n",
        "    print(\"Coreset data shape:\", X_coreset.shape)\n",
        "    print(\"Number of features:\", X_coreset.shape[1])\n",
        "    coreset_construction_time = time.time() - start_time\n",
        "    print(f\"Coreset construction time (s): {coreset_construction_time}\")\n",
        "    print(f\"Coreset size: {X_coreset.shape[0]}\")\n",
        "    print(f\"Unique classes in coreset: {y_coreset.nunique()}\")\n",
        "    if y_coreset.nunique() < 2:\n",
        "        raise ValueError(\"Coreset contains only one class. Adjust the number of clusters or check data diversity.\")\n",
        "    start_time = time.time()\n",
        "    clf = LogisticRegression(max_iter=1000, n_jobs=-1)\n",
        "    clf.fit(X_coreset, y_coreset)\n",
        "    end_time = time.time()\n",
        "    print(f\"Coreset training time (s): {end_time - start_time}\")\n",
        "    check_model_metrics(clf, X_test, y_test)\n",
        "\n",
        "train_full_data(df_train, df_test)\n",
        "train_downsampled_data(df_train, df_test)\n",
        "train_with_coreset(df_train, df_test)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "90gjj5d0haFc",
        "outputId": "286a7d7f-348f-4534-b3b2-f0a120629a04"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Combined DataFrame shape: (1517041, 2)\n",
            "Class distribution:\n",
            " label\n",
            "0    763629\n",
            "4    753412\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Training Data Shape: (1213632, 2)\n",
            "Test Data Shape: (303409, 2)\n",
            "\n",
            "Experiment 1: Direct HashingVectorizer\n",
            "Training data shape: (1213632, 262144)\n",
            "Number of features: 262144\n",
            "Direct training time (s): 27.637704610824585\n",
            "ACCURACY:\n",
            "77.68292964282536\n",
            "\n",
            "CONFUSION MATRIX\n",
            "[[116102  36624]\n",
            " [ 31088 119595]]\n",
            "\n",
            "CLASSIFICATION REPORT\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.79      0.76      0.77    152726\n",
            "           4       0.77      0.79      0.78    150683\n",
            "\n",
            "    accuracy                           0.78    303409\n",
            "   macro avg       0.78      0.78      0.78    303409\n",
            "weighted avg       0.78      0.78      0.78    303409\n",
            "\n",
            "\n",
            "Experiment 2: Stratified Downsampling and Reduced Features\n",
            "Downsampled training data shape: (100000, 1024)\n",
            "Number of features: 1024\n",
            "Downsampled training time (s): 1.272170066833496\n",
            "ACCURACY:\n",
            "69.97748913183194\n",
            "\n",
            "CONFUSION MATRIX\n",
            "[[105902  46824]\n",
            " [ 44267 106416]]\n",
            "\n",
            "CLASSIFICATION REPORT\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.71      0.69      0.70    152726\n",
            "           4       0.69      0.71      0.70    150683\n",
            "\n",
            "    accuracy                           0.70    303409\n",
            "   macro avg       0.70      0.70      0.70    303409\n",
            "weighted avg       0.70      0.70      0.70    303409\n",
            "\n",
            "\n",
            "Experiment 3: Proper Coreset Construction\n",
            "Constructing coreset with 1000 clusters...\n",
            "Sampled data shape before coreset construction: (100000, 1024)\n",
            "Number of features: 1024\n",
            "Coreset data shape: (866, 1024)\n",
            "Number of features: 1024\n",
            "Coreset construction time (s): 48.3636417388916\n",
            "Coreset size: 866\n",
            "Unique classes in coreset: 2\n",
            "Coreset training time (s): 0.023307085037231445\n",
            "ACCURACY:\n",
            "64.5946560583239\n",
            "\n",
            "CONFUSION MATRIX\n",
            "[[ 88944  63782]\n",
            " [ 43641 107042]]\n",
            "\n",
            "CLASSIFICATION REPORT\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.67      0.58      0.62    152726\n",
            "           4       0.63      0.71      0.67    150683\n",
            "\n",
            "    accuracy                           0.65    303409\n",
            "   macro avg       0.65      0.65      0.64    303409\n",
            "weighted avg       0.65      0.65      0.64    303409\n",
            "\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}